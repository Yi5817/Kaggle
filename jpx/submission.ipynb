{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-04T17:12:08.232916Z","iopub.execute_input":"2022-07-04T17:12:08.233608Z","iopub.status.idle":"2022-07-04T17:12:08.251927Z","shell.execute_reply.started":"2022-07-04T17:12:08.233574Z","shell.execute_reply":"2022-07-04T17:12:08.250494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nfrom time import time\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.255351Z","iopub.execute_input":"2022-07-04T17:12:08.256209Z","iopub.status.idle":"2022-07-04T17:12:08.26707Z","shell.execute_reply.started":"2022-07-04T17:12:08.256135Z","shell.execute_reply":"2022-07-04T17:12:08.265927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.272868Z","iopub.execute_input":"2022-07-04T17:12:08.273496Z","iopub.status.idle":"2022-07-04T17:12:08.283985Z","shell.execute_reply.started":"2022-07-04T17:12:08.273468Z","shell.execute_reply":"2022-07-04T17:12:08.282454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"def options_FE(df_day,date):\n    total_volume = df_day.TradingVolume.sum()\n    df_put = df_day[df_day.Putcall==1]\n    put_volume = df_put.TradingVolume.sum()\n    call_volume = total_volume-put_volume\n    put_ratio = put_volume/total_volume\n    total_volume_day = df_day.WholeDayVolume.sum()\n    put_volume_day = df_put.WholeDayVolume.sum()\n    call_volume_day = total_volume_day-put_volume_day\n    put_ratio_day = put_volume_day/total_volume_day\n    call_call_ratio = call_volume_day/call_volume\n    put_put_ratio = put_volume_day/put_volume\n    df_day['rel_vol'] = df_day['ImpliedVolatility']/df_day['BaseVolatility']\n    avg_vol = np.nanmean(df_day['rel_vol'])\n    med_vol = np.nanmedian(df_day['rel_vol'])\n    std_vol = np.nanstd(df_day['rel_vol'])    \n    dividend, IR, DR, vol = df_day[['Dividend','InterestRate','DividendRate','BaseVolatility']].mean()\n    \n    return [date, total_volume, \n            put_ratio,\n            total_volume_day,\n            put_ratio_day,\n            call_call_ratio,\n            put_put_ratio,\n            avg_vol,\n            med_vol,\n            std_vol, \n            dividend, IR, DR, vol]\n\noptions_feature_names = ['date','total_volume', \n            'put_ratio',\n            'total_volume_day',\n            'put_ratio_day',\n            'call_call_ratio',\n            'put_put_ratio',\n            'avg_vol',\n            'med_vol',\n            'std_vol', \n            'dividend', 'IR', 'DR', 'vol']\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.289122Z","iopub.execute_input":"2022-07-04T17:12:08.290084Z","iopub.status.idle":"2022-07-04T17:12:08.302035Z","shell.execute_reply.started":"2022-07-04T17:12:08.290042Z","shell.execute_reply":"2022-07-04T17:12:08.300799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def options_feature(options):\n    options_grouped = options.groupby('Date')\n    \n    list_df = [] \n\n    for date, df in tqdm(options_grouped):\n        list_df.append(options_FE(df,date))\n\n    df_result = pd.DataFrame(np.array(list_df),columns=options_feature_names).set_index('date').astype('float32')\n    df_result.index = pd.to_datetime(df_result.index, format=\"%Y-%m-%d\")\n    \n    del options_grouped\n    del list_df\n    #gc.collect()\n    \n    return df_result","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.307848Z","iopub.execute_input":"2022-07-04T17:12:08.308157Z","iopub.status.idle":"2022-07-04T17:12:08.319653Z","shell.execute_reply.started":"2022-07-04T17:12:08.308133Z","shell.execute_reply":"2022-07-04T17:12:08.318178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trades","metadata":{}},{"cell_type":"code","source":"def gen_dates(b_date, days):\n    day = timedelta(days=1)\n    for i in range(days):\n        yield b_date + day*i\n\n\ndef get_date_list(start=None, end=None):\n\n    if start is None:\n        start = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\")\n    if end is None:\n        end = datetime.now()\n    data = []\n    for d in gen_dates(start, (end-start).days+1):\n        data.append(d)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.325447Z","iopub.execute_input":"2022-07-04T17:12:08.325902Z","iopub.status.idle":"2022-07-04T17:12:08.335765Z","shell.execute_reply.started":"2022-07-04T17:12:08.325875Z","shell.execute_reply":"2022-07-04T17:12:08.333801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_date(df):\n    df_sales = df.TotalSales.diff().dropna()\n    df_purchases = df.TotalPurchases.diff().dropna()\n    df_total = df.TotalTotal.diff().dropna()\n    df_balance = df.TotalBalance.diff().dropna()\n    df_sales.reset_index(drop=True, inplace=True)\n    df_purchases.reset_index(drop=True, inplace=True)\n    df_total.reset_index(drop=True, inplace=True)\n    df_balance.reset_index(drop=True, inplace=True)\n    df_sales_ratio = df_sales/df.TotalSales[:-1].tolist()\n    df_purchases_ratio = df_purchases/df.TotalPurchases[:-1].tolist()\n    df_total_ratio = df_total/df.TotalTotal[:-1].tolist()\n    df_balance_ratio = df_balance/df.TotalBalance[:-1].tolist()\n    date_value = np.zeros((1,5))\n    for i in range(1,df.shape[0]):\n        date_list = get_date_list(df.loc[i,\"StartDate\"],df.loc[i,\"EndDate\"])\n        value_list = [df_sales_ratio[i-1],df_purchases_ratio[i-1],df_total_ratio[i-1],df_balance_ratio[i-1]]\n        values = np.tile(np.array(value_list).reshape(1,-1),(len(date_list),1))\n        dates = np.array(date_list).reshape(-1,1)\n        date_value_sub = np.hstack((dates,values))\n        date_value = np.vstack((date_value,date_value_sub))\n    date_value_df = pd.DataFrame(date_value[1:,:],index=None,columns=['Date','TotalSales','TotalPurchases','TotalTotal','TotalBalance'])\n    date_value_df = date_value_df.drop_duplicates(subset=['Date'])\n    \n    del df_sales, df_purchases, df_total, df_balance, df_sales_ratio, df_purchases_ratio, df_total_ratio, df_balance_ratio\n    del date_value, date_list, value_list, values, dates, date_value_sub\n    #gc.collect()\n    \n    return date_value_df","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.340155Z","iopub.execute_input":"2022-07-04T17:12:08.341231Z","iopub.status.idle":"2022-07-04T17:12:08.359383Z","shell.execute_reply.started":"2022-07-04T17:12:08.341169Z","shell.execute_reply":"2022-07-04T17:12:08.357934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trades_feature(trades):\n    trades_sub = trades[pd.isna(trades[\"StartDate\"]) != True]\n    trades_sub.reset_index(drop=True, inplace=True)\n    trades_subset = trades_sub.loc[:,'StartDate':'TotalBalance']\n    trades_subset[\"StartDate\"] = pd.to_datetime(trades_subset[\"StartDate\"], format=\"%Y-%m-%d\")\n    trades_subset[\"EndDate\"] = pd.to_datetime(trades_subset[\"EndDate\"], format=\"%Y-%m-%d\")\n    trades_subset[\"StartDate\"] = trades_subset[\"StartDate\"].dt.date\n    trades_subset[\"EndDate\"] = trades_subset[\"EndDate\"].dt.date\n    growth = trades_subset[trades_subset[\"Section\"] == \"Growth Market (Mothers/JASDAQ)\"]\n    prime = trades_subset[trades_subset[\"Section\"] == \"Prime Market (First Section)\"]\n    standard = trades_subset[trades_subset[\"Section\"] == \"Standard Market (Second Section)\"]\n    growth.reset_index(drop=True, inplace=True)\n    prime.reset_index(drop=True, inplace=True)\n    standard.reset_index(drop=True, inplace=True)\n    growth_processed = fill_date(growth)\n    prime_processed = fill_date(prime)\n    standard_processed = fill_date(standard)\n    \n    del trades_sub, trades_subset, growth, prime, standard\n    #gc.collect()\n    \n    return growth_processed, prime_processed, standard_processed\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.361855Z","iopub.execute_input":"2022-07-04T17:12:08.362493Z","iopub.status.idle":"2022-07-04T17:12:08.376897Z","shell.execute_reply.started":"2022-07-04T17:12:08.36245Z","shell.execute_reply":"2022-07-04T17:12:08.374899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stock features","metadata":{}},{"cell_type":"code","source":"def RSI(series, period):\n    delta = series.diff().dropna()\n    u = delta * 0\n    d = u.copy()\n    u[delta > 0] = delta[delta > 0]\n    d[delta < 0] = -delta[delta < 0]\n    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n    u = u.drop(u.index[:(period-1)])\n    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n    d = d.drop(d.index[:(period-1)])\n    rs = pd.DataFrame.ewm(u, com=period-1, adjust=False).mean() / \\\n         pd.DataFrame.ewm(d, com=period-1, adjust=False).mean()\n    \n    del delta, u, d\n    #gc.collect()\n    \n    return 100 - 100 / (1 + rs)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.379515Z","iopub.execute_input":"2022-07-04T17:12:08.380988Z","iopub.status.idle":"2022-07-04T17:12:08.394324Z","shell.execute_reply.started":"2022-07-04T17:12:08.380878Z","shell.execute_reply":"2022-07-04T17:12:08.392849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MACD_add(data_fct,N = [12,26], M = 9):\n    \n   \n    time_now = time()\n    \n    data = copy.deepcopy(data_fct)\n    \n \n    output = copy.deepcopy(data_fct)\n\n    EMA1 = data['Close'].ewm(span = N[0], adjust = False).mean()\n    EMA2 = data['Close'].ewm(span = N[1], adjust = False).mean()\n    DIFF = EMA1 - EMA2\n    DEA = DIFF.ewm(span = M, adjust = False).mean()\n    \n\n    MACD = 2*(DIFF - DEA)\n    MACD.rename('MACD',inplace = True)\n    DIFF.rename('DIFF',inplace = True)\n    DEA.rename('DEA',inplace = True)\n    \n    output = output.merge(DIFF,left_index = True,right_index = True, how = 'left')\n    output = output.merge(DEA,left_index = True,right_index = True, how = 'left')\n    output = output.merge(MACD,left_index = True,right_index = True, how = 'left')\n    \n    del data, EMA1, EMA2, DIFF, DEA, MACD\n    #gc.collect()\n    \n    return(output)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.396983Z","iopub.execute_input":"2022-07-04T17:12:08.397574Z","iopub.status.idle":"2022-07-04T17:12:08.411952Z","shell.execute_reply.started":"2022-07-04T17:12:08.39753Z","shell.execute_reply":"2022-07-04T17:12:08.410449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def KDJ_add(data_fct,N = 9):\n\n    time_now = time()\n  \n    def _KDJ_f(series):\n        kdj = series.copy()\n        for i in range(len(series)):\n            if i == 0:\n                kdj[i] = 50\n            else:\n                kdj[i] = 2/3 * kdj[i-1] + 1/3 * series[i]\n        return(kdj)\n  \n    data = copy.deepcopy(data_fct)\n\n    Lowest = data['Low'].rolling(N).min()\n    Highest = data['High'].rolling(N).max()\n    RSV = (data['Close'] - Lowest)/(Highest - Lowest)*100\n    RSV.dropna(inplace = True)\n  \n\n    KDJ_K = _KDJ_f(RSV)\n    KDJ_K.rename('KDJ_K',inplace = True)\n    KDJ_D = _KDJ_f(KDJ_K)\n    KDJ_D.rename('KDJ_D',inplace = True)\n    KDJ_J = 3 * KDJ_D - 2 * KDJ_K\n    KDJ_J.rename('KDJ_J',inplace = True)\n\n\n    output = copy.deepcopy(data_fct)\n\n    output = output.merge(KDJ_K,left_index = True,right_index = True,how = 'left')\n    \n    output = output.merge(KDJ_D,left_index = True,right_index = True,how = 'left')\n    \n    output = output.merge(KDJ_J,left_index = True,right_index = True,how = 'left')\n  \n\n    \n    del data, Lowest, Highest, RSV, KDJ_K, KDJ_D, KDJ_J\n    #gc.collect()\n    \n    return(output)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.415297Z","iopub.execute_input":"2022-07-04T17:12:08.416119Z","iopub.status.idle":"2022-07-04T17:12:08.431304Z","shell.execute_reply.started":"2022-07-04T17:12:08.41608Z","shell.execute_reply":"2022-07-04T17:12:08.42992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Boll_add(data_fct,N = 20):\n    \n  \n    time_now = time()\n\n    data = copy.deepcopy(data_fct)\n    \n\n    MA = data['Close'].rolling(N).mean()\n    Std = data['Close'].rolling(N).std()\n    \n\n    BollUp = MA + 2*Std \n    BollUp.rename('BollUp',inplace = True)\n    \n\n    BollDown = MA - 2*Std \n    BollDown.rename('BollDown',inplace = True)\n    \n\n    output = copy.deepcopy(data_fct)\n\n    output = output.merge(BollUp,left_index = True,right_index = True, how = 'left')\n\n    output = output.merge(BollDown,left_index = True,right_index = True, how = 'left')\n    \n\n    del data, MA, Std, BollUp, BollDown\n    #gc.collect()\n    \n    return(output)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.433181Z","iopub.execute_input":"2022-07-04T17:12:08.433696Z","iopub.status.idle":"2022-07-04T17:12:08.44475Z","shell.execute_reply.started":"2022-07-04T17:12:08.433656Z","shell.execute_reply":"2022-07-04T17:12:08.443433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"adjust price","metadata":{}},{"cell_type":"code","source":"def calculate_adjusted_prices(df_security):\n    \n    df_security.fillna(method='ffill', inplace=True)\n    \n    df_security[['Open', 'High', 'Low', 'Close']]= df_security[['Open', 'High', 'Low', 'Close']]\\\n        .multiply(\n            df_security.AdjustmentFactor.sort_index(ascending=False)\\\n                .cumprod()\\\n                .sort_index(ascending=True), \n            axis=0)\n    return df_security","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.448042Z","iopub.execute_input":"2022-07-04T17:12:08.448925Z","iopub.status.idle":"2022-07-04T17:12:08.460805Z","shell.execute_reply.started":"2022-07-04T17:12:08.448883Z","shell.execute_reply":"2022-07-04T17:12:08.459478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"padding","metadata":{}},{"cell_type":"code","source":"def padding(array,max_len):\n    if array.shape[0] < max_len:\n        initial = array[0,:].reshape(1,-1)\n        pad_np = np.tile(initial,(max_len-array.shape[0],1))\n        array_new = np.vstack((pad_np,array))\n        \n        del initial, pad_np\n        \n    else:\n        array_new = array\n    \n    \n    gc.collect()\n    \n    return array_new","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.463121Z","iopub.execute_input":"2022-07-04T17:12:08.46375Z","iopub.status.idle":"2022-07-04T17:12:08.474034Z","shell.execute_reply.started":"2022-07-04T17:12:08.463706Z","shell.execute_reply":"2022-07-04T17:12:08.472377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split data","metadata":{}},{"cell_type":"code","source":"def split_data(stock, lookback):\n    #data_raw = stock.to_numpy() # convert to numpy array\n    data = []\n    \n    # create all possible sequences of length seq_len\n    for index in range(stock.shape[0] - lookback + 1): \n        data.append(stock[index: index + lookback])\n    \n    data = np.array(data)\n    #test_set_size = int(np.round(0.2*data.shape[0]))\n    #train_set_size = data.shape[0] - (test_set_size)\n    \n    x_train = data[:,:-1,:]\n    y_train = data[:,-1,-1].reshape(-1,1,)\n    \n    del data\n    gc.collect()\n    \n    return [x_train, y_train]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.477544Z","iopub.execute_input":"2022-07-04T17:12:08.4781Z","iopub.status.idle":"2022-07-04T17:12:08.488973Z","shell.execute_reply.started":"2022-07-04T17:12:08.478055Z","shell.execute_reply":"2022-07-04T17:12:08.486926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_processing(stock_prices,options_all,trades_all):\n    # stock_price\n    stock_prices_copy = stock_prices.set_index('Date',inplace = False)\n    stock_prices_copy.index = pd.to_datetime(stock_prices_copy.index, format=\"%Y-%m-%d\")\n    stock_prices_copy['SupervisionFlag'] = stock_prices_copy['SupervisionFlag'].map({True: 1, False: 0})\n    stock_ids = stock_prices_copy.SecuritiesCode.unique()\n    stocks_list = []\n    for stock_id in stock_ids:\n        stock = stock_prices_copy[stock_prices_copy[\"SecuritiesCode\"]==stock_id]\n        stock = calculate_adjusted_prices(stock)\n        stocks_list.append(stock)\n    stocks = np.array(stocks_list)\n    del stocks_list\n    \n    \n    # add RSI\n    for i in range(len(stocks)):\n        stocks[i]['RSI'] = RSI(stocks[i]['Close'], 14)\n    # add MACD\n    for i in range(len(stocks)):\n        stocks[i] = MACD_add(stocks[i])\n    # add KDJ\n    for i in range(len(stocks)):\n        stocks[i] = KDJ_add(stocks[i])\n    # add BOLL\n    for i in range(len(stocks)):\n        stocks[i] = Boll_add(stocks[i])\n        \n    # add options\n    options_pro = options_feature(options_all)\n    for i in range(len(stocks)):\n        stocks[i] = stocks[i].merge(options_pro,left_index = True,right_index = True, how = 'left')\n        \n    # add Trades\n    growth, prime, standard = trades_feature(trades_all)\n    prime.set_index('Date',inplace = True)\n    prime.index = pd.to_datetime(prime.index, format=\"%Y-%m-%d\")\n    standard.set_index('Date',inplace = True)\n    standard.index = pd.to_datetime(standard.index, format=\"%Y-%m-%d\")\n    growth.set_index('Date',inplace = True)\n    growth.index = pd.to_datetime(growth.index, format=\"%Y-%m-%d\")\n    for i in range(len(stocks)):\n        s_code = stocks[i].SecuritiesCode.unique()[0]\n        market = stock_list[stock_list[\"SecuritiesCode\"]==s_code].NewMarketSegment.to_list()[0]\n        if 'Prime Market' in market:\n            stocks[i] = stocks[i].merge(prime,left_index = True,right_index = True, how = 'left')\n        if 'Standard Market' in market:\n            stocks[i] = stocks[i].merge(standard,left_index = True,right_index = True, how = 'left')\n        if 'Growth Market' in market:\n            stocks[i] = stocks[i].merge(growth,left_index = True,right_index = True, how = 'left') \n    \n    del stock_prices_copy\n    del options_pro\n    del growth\n    del prime\n    del standard\n    gc.collect()\n\n    return stocks, stock_ids","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.493241Z","iopub.execute_input":"2022-07-04T17:12:08.494017Z","iopub.status.idle":"2022-07-04T17:12:08.514179Z","shell.execute_reply.started":"2022-07-04T17:12:08.493901Z","shell.execute_reply":"2022-07-04T17:12:08.512549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"markdown","source":"Load Data and add features","metadata":{}},{"cell_type":"code","source":"# Load Data\nstock_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\noptions_all = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/options.csv\")\ntrades_all = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/trades.csv\")\nstock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\n\nstock_prices_sup = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\noptions_sup = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/options.csv\")\ntrades_sup = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/trades.csv\")\n\nstock_prices = pd.concat([stock_prices,stock_prices_sup])\noptions_all = pd.concat([options_all,options_sup])\ntrades_all = pd.concat([trades_all,trades_sup])","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:08.519391Z","iopub.execute_input":"2022-07-04T17:12:08.519922Z","iopub.status.idle":"2022-07-04T17:12:30.467778Z","shell.execute_reply.started":"2022-07-04T17:12:08.519889Z","shell.execute_reply":"2022-07-04T17:12:30.466438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del stock_prices_sup\ndel options_sup\ndel trades_sup\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:30.470612Z","iopub.execute_input":"2022-07-04T17:12:30.471323Z","iopub.status.idle":"2022-07-04T17:12:30.61378Z","shell.execute_reply.started":"2022-07-04T17:12:30.471241Z","shell.execute_reply":"2022-07-04T17:12:30.61247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stocks, stock_ids = data_processing(stock_prices,options_all,trades_all)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:12:30.615454Z","iopub.execute_input":"2022-07-04T17:12:30.616847Z","iopub.status.idle":"2022-07-04T17:15:28.197943Z","shell.execute_reply.started":"2022-07-04T17:12:30.616787Z","shell.execute_reply":"2022-07-04T17:15:28.196635Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data processing and PCA","metadata":{}},{"cell_type":"code","source":"# Drop columns\nfor i in range(len(stocks)):\n    stocks[i] = stocks[i].drop(columns=['RowId', 'SecuritiesCode','AdjustmentFactor','ExpectedDividend', 'Target'])\n\n# Scale data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(-1,1))\ncolumn_list = stocks[0].columns\nfor i in range(len(stocks)):\n    for column in column_list:\n        stocks[i][column] = scaler.fit_transform(stocks[i][column].values.reshape(-1,1))\n        \n# fill nan\nfor i in range(len(stocks)):\n    stocks[i] = stocks[i].fillna(-1)\n    \n# PCA\nfrom sklearn.decomposition import PCA\nfor i in range(len(stocks)):\n    feature_1 = stocks[i][['Open', 'High', 'Low', 'Close', 'Volume', 'SupervisionFlag', 'RSI', 'DIFF', 'DEA', 'MACD',\n       'KDJ_K', 'KDJ_D', 'KDJ_J', 'BollUp', 'BollDown']]\n    pca_1 = PCA(n_components=3)\n    pca_1.fit(feature_1)\n    PC_1 = pca_1.fit_transform(feature_1)\n    feature_2 = stocks[i][['total_volume',\n       'put_ratio', 'total_volume_day', 'put_ratio_day', 'call_call_ratio',\n       'put_put_ratio', 'avg_vol', 'med_vol', 'std_vol', 'dividend', 'IR',\n       'DR', 'vol']]\n    pca_2 = PCA(n_components=4)\n    pca_2.fit(feature_2)\n    PC_2 = pca_2.fit_transform(feature_2)\n    feature_3 = stocks[i][['TotalSales', 'TotalPurchases', 'TotalTotal',\n       'TotalBalance']]\n    pca_3 = PCA(n_components=2)\n    pca_3.fit(feature_3)\n    PC_3 = pca_3.fit_transform(feature_3)\n    stocks[i] = np.hstack((PC_1,PC_2,PC_3)) \n\n# add Target\nfor i in range(len(stocks)):\n    s_code = stock_ids[i]\n    target = stock_prices[stock_prices[\"SecuritiesCode\"]==s_code][[\"Target\"]].reset_index(drop = True).fillna(-1).values.reshape(-1,1)\n    stocks[i] = np.hstack((stocks[i],target))\n    \n# padding\nmax_len = stocks[0].shape[0]\nfor i in range(len(stocks)):\n    stocks[i] = padding(stocks[i],max_len)\n    \n# Make a few changes\nstocks = stocks.tolist()\nstocks = np.array(stocks, dtype=np.float64)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:15:28.205808Z","iopub.execute_input":"2022-07-04T17:15:28.208608Z","iopub.status.idle":"2022-07-04T17:21:11.063454Z","shell.execute_reply.started":"2022-07-04T17:15:28.208566Z","shell.execute_reply":"2022-07-04T17:21:11.062152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_prices.to_csv(\"./stock_prices.csv\",index=False)\noptions_all.to_csv(\"./options_all.csv\",index=False)\ntrades_all.to_csv(\"./trades_all.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:21:11.067403Z","iopub.execute_input":"2022-07-04T17:21:11.067843Z","iopub.status.idle":"2022-07-04T17:22:50.3779Z","shell.execute_reply.started":"2022-07-04T17:21:11.067782Z","shell.execute_reply":"2022-07-04T17:22:50.37668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del stock_prices, options_all, trades_all, stock_list\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:22:50.379596Z","iopub.execute_input":"2022-07-04T17:22:50.380487Z","iopub.status.idle":"2022-07-04T17:22:50.812861Z","shell.execute_reply.started":"2022-07-04T17:22:50.380447Z","shell.execute_reply":"2022-07-04T17:22:50.811393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data\nlookback = 14\nx_train_list = []\ny_train_list = []\n\nfor i in range(stocks.shape[0]):\n    x_train, y_train = split_data(stocks[i,:,:], lookback)\n    x_train_list.append(x_train)\n    y_train_list.append(y_train)\n    \n# switch to tensor\nX_train = np.array(x_train_list, dtype=np.float64)\nX_train_tensor = torch.from_numpy(X_train)\ny_train = np.array(y_train_list, dtype=np.float64)\ny_train_tensor = torch.from_numpy(y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:22:50.814691Z","iopub.execute_input":"2022-07-04T17:22:50.815406Z","iopub.status.idle":"2022-07-04T17:27:04.682403Z","shell.execute_reply.started":"2022-07-04T17:22:50.815364Z","shell.execute_reply":"2022-07-04T17:27:04.68109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train\ndel y_train\ndel x_train_list\ndel y_train_list\ndel stocks, stock_ids\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:04.684139Z","iopub.execute_input":"2022-07-04T17:27:04.684564Z","iopub.status.idle":"2022-07-04T17:27:04.875138Z","shell.execute_reply.started":"2022-07-04T17:27:04.684522Z","shell.execute_reply":"2022-07-04T17:27:04.873308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"class stock_Dataset(Dataset):\n    def __init__(self,X,y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        self.len = len(self.X)\n        return self.len\n\n    def __getitem__(self, i):\n        X_sub = self.X[i]\n        y_sub = self.y[i]\n        return X_sub,y_sub\n    \nbatch_size = 64\ntrain_dataset = stock_Dataset(X_train_tensor,y_train_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n#test_dataset = stock_Dataset(X_test_tensor,y_test_tensor)\n#test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:04.877949Z","iopub.execute_input":"2022-07-04T17:27:04.878469Z","iopub.status.idle":"2022-07-04T17:27:04.89238Z","shell.execute_reply.started":"2022-07-04T17:27:04.878365Z","shell.execute_reply":"2022-07-04T17:27:04.890902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSTM Model","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        self.num_classes = num_classes #number of classes\n        self.num_layers = num_layers #number of layers\n        self.input_size = input_size #input size\n        self.hidden_size = hidden_size #hidden state\n\n        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size,\n                          num_layers=self.num_layers, batch_first=True) #lstm\n        self.fc =  nn.Linear(self.hidden_size, self.num_classes)\n    \n    def forward(self,x):\n        #h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_() #hidden state\n        #c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_() #internal state\n        #print(h_0.shape)\n        #print(c_0.shape)\n        # Propagate input through LSTM\n        #out, (hn, cn) = self.lstm(x, (h_0.detach(), c_0.detach())) #lstm with input, hidden, and internal state\n        #print(x.shape)\n        out, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n        out = self.fc(out[:,-1,:]) #Final Output\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:04.89941Z","iopub.execute_input":"2022-07-04T17:27:04.899887Z","iopub.status.idle":"2022-07-04T17:27:04.913162Z","shell.execute_reply.started":"2022-07-04T17:27:04.899857Z","shell.execute_reply":"2022-07-04T17:27:04.911842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, model, criterion, optimizer):\n    epoch_loss = 0\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X,y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        loss = 0\n        for i in range(len(X)):\n            x = X[i]\n            pred = model(x)\n            loss_sub = criterion(pred,y[i])\n            loss += loss_sub/len(X)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()*len(X)\n\n    epoch_loss = epoch_loss/size\n    print(\"loss: %f\" % epoch_loss)\n    \n    del batch, size, epoch_loss, loss_sub, loss","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:04.916512Z","iopub.execute_input":"2022-07-04T17:27:04.917501Z","iopub.status.idle":"2022-07-04T17:27:04.92861Z","shell.execute_reply.started":"2022-07-04T17:27:04.917457Z","shell.execute_reply":"2022-07-04T17:27:04.927336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 200 #1000 epochs\nlearning_rate = 0.00005 #0.001 lr\ninput_size = 10 #number of features\nhidden_size = 30 #number of features in hidden state\nnum_layers = 2 #number of stacked lstm layers\nnum_classes = 1 #number of output classes \n\nmodel = LSTM(num_classes, input_size, hidden_size, num_layers).to(device)\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) ","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:04.930925Z","iopub.execute_input":"2022-07-04T17:27:04.931391Z","iopub.status.idle":"2022-07-04T17:27:09.399486Z","shell.execute_reply.started":"2022-07-04T17:27:04.931352Z","shell.execute_reply":"2022-07-04T17:27:09.398322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model = model.double()\n    print(\"Epoch: %d\" % epoch)\n    train(train_dataloader, model, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:09.401116Z","iopub.execute_input":"2022-07-04T17:27:09.401813Z","iopub.status.idle":"2022-07-04T17:27:16.897194Z","shell.execute_reply.started":"2022-07-04T17:27:09.401757Z","shell.execute_reply":"2022-07-04T17:27:16.895935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_dataset\ndel train_dataloader\ndel X_train_tensor\ndel y_train_tensor\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:16.89947Z","iopub.execute_input":"2022-07-04T17:27:16.899973Z","iopub.status.idle":"2022-07-04T17:27:17.135571Z","shell.execute_reply.started":"2022-07-04T17:27:16.899916Z","shell.execute_reply":"2022-07-04T17:27:17.134129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del criterion, optimizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:17.13768Z","iopub.execute_input":"2022-07-04T17:27:17.138484Z","iopub.status.idle":"2022-07-04T17:27:17.270997Z","shell.execute_reply.started":"2022-07-04T17:27:17.138438Z","shell.execute_reply":"2022-07-04T17:27:17.269526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test files","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:17.273621Z","iopub.execute_input":"2022-07-04T17:27:17.274445Z","iopub.status.idle":"2022-07-04T17:27:17.297246Z","shell.execute_reply.started":"2022-07-04T17:27:17.274397Z","shell.execute_reply":"2022-07-04T17:27:17.296051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    print(\"count = \", count)\n    predictions = {}\n    \n    stock_prices = pd.read_csv(\"./stock_prices.csv\")\n    options_all = pd.read_csv(\"./options_all.csv\")\n    trades_all = pd.read_csv(\"./trades_all.csv\")\n    stock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\n    \n    test_date = prices.Date.unique()[0]\n    stock_prices = stock_prices[stock_prices[\"Date\"]<test_date]\n    options_all = options_all[options_all[\"Date\"]<test_date]\n    trades_all = trades_all[trades_all[\"Date\"]<test_date]\n    \n    stocks, stock_ids = data_processing(stock_prices,options_all,trades_all)\n    \n    # Drop columns\n    for i in range(len(stocks)):\n        stocks[i] = stocks[i].drop(columns=['RowId', 'SecuritiesCode','AdjustmentFactor','ExpectedDividend','Target'])\n\n    # Scale data\n    scaler = MinMaxScaler(feature_range=(-1,1))\n    column_list = stocks[0].columns\n    for i in range(len(stocks)):\n        for column in column_list:\n            stocks[i][column] = scaler.fit_transform(stocks[i][column].values.reshape(-1,1))\n\n    # fill nan\n    for i in range(len(stocks)):\n        stocks[i] = stocks[i].fillna(-1)\n\n    # PCA\n    for i in range(len(stocks)):\n        feature_1 = stocks[i][['Open', 'High', 'Low', 'Close', 'Volume', 'SupervisionFlag', 'RSI', 'DIFF', 'DEA', 'MACD',\n       'KDJ_K', 'KDJ_D', 'KDJ_J', 'BollUp', 'BollDown']]\n        PC_1 = pca_1.transform(feature_1)\n        feature_2 = stocks[i][['total_volume',\n       'put_ratio', 'total_volume_day', 'put_ratio_day', 'call_call_ratio',\n       'put_put_ratio', 'avg_vol', 'med_vol', 'std_vol', 'dividend', 'IR',\n       'DR', 'vol']]\n        PC_2 = pca_2.transform(feature_2)\n        feature_3 = stocks[i][['TotalSales', 'TotalPurchases', 'TotalTotal',\n       'TotalBalance']]\n        PC_3 = pca_3.transform(feature_3)\n        stocks[i] = np.hstack((PC_1,PC_2,PC_3)) \n\n    # add Target\n    for i in range(len(stocks)):\n        s_code = stock_ids[i]\n        target = stock_prices[stock_prices[\"SecuritiesCode\"]==s_code][[\"Target\"]].reset_index(drop = True).fillna(-1).values.reshape(-1,1)\n        stocks[i] = np.hstack((stocks[i],target))\n        del target, s_code\n    \n\n\n    # padding\n    max_len = stocks[0].shape[0]\n    for i in range(len(stocks)):\n        stocks[i] = padding(stocks[i],max_len)\n\n    # Make a few changes\n    stocks = stocks.tolist()\n    stocks = np.array(stocks, dtype=np.float64)\n    \n    gc.collect()\n    \n    for i in range(stocks.shape[0]):\n        X_test = stocks[i,:,:].reshape(1,stocks[i,:,:].shape[0],-1)\n        X_test = X_test[:,-lookback+1:,:]\n        X_tensor = torch.from_numpy(X_test).to(device)\n        pred = model(X_tensor)\n        predictions[stock_ids[i]] = pred.detach().cpu().numpy().flatten()[0]\n        del X_test, X_tensor, pred\n        \n    prices[\"Target\"] = prices[\"SecuritiesCode\"].map(predictions) \n    prices_new = prices.copy()\n    prices_new[\"Rank\"] = prices_new.groupby(\"Date\")[\"Target\"].rank(ascending=False, method=\"first\") - 1 \n    prices_new[\"Rank\"] = prices_new[\"Rank\"].astype(\"int\")\n    pred_rank = prices_new.set_index(\"SecuritiesCode\")[\"Rank\"]\n    sample_prediction['Rank'] = sample_prediction[\"SecuritiesCode\"].map(pred_rank)\n    del prices_new\n    del pred_rank\n    del stocks, stock_ids\n    gc.collect()\n    \n    if test_date > stock_prices.Date.unique()[-1]:\n        stock_prices = pd.concat([stock_prices,prices]).reset_index(drop = True)\n        options_all = pd.concat([options_all,options]).reset_index(drop = True)\n        trades_all = pd.concat([trades_all,trades]).reset_index(drop = True)\n    \n    del prices, options, trades, test_date\n    \n    stock_prices.to_csv(\"./stock_prices.csv\",index=False)\n    options_all.to_csv(\"./options_all.csv\",index=False)\n    trades_all.to_csv(\"./trades_all.csv\",index=False)\n    del stock_prices, options_all, trades_all, stock_list\n    gc.collect()\n    \n    env.predict(sample_prediction)\n    count += 1","metadata":{"execution":{"iopub.status.busy":"2022-07-04T17:27:17.299484Z","iopub.execute_input":"2022-07-04T17:27:17.300006Z","iopub.status.idle":"2022-07-04T17:47:19.306919Z","shell.execute_reply.started":"2022-07-04T17:27:17.299962Z","shell.execute_reply":"2022-07-04T17:47:19.305418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}